{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d38fba5c",
   "metadata": {},
   "source": [
    "# Predicting ad click-through with a decision tree "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440d5831",
   "metadata": {},
   "source": [
    "We will use the dataset from a Kaggle machine learning competition, Click-Through Rate Prediction (https://www.kaggle.com/c/avazu-ctr-prediction). The dataset can be downloaded from https://www.kaggle.com/c/avazu-ctr-prediction/data.\n",
    "Only the train.gz file contains labeled samples, so we only need to download this and unzip it (it will take a while)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fb0be2",
   "metadata": {},
   "source": [
    "Values are anonymized and hashed because they are categorical features, and each of their possible values corresponds to a real and meaningful value, but it is presented this way due to the privacy policy. Possibly, C1 means user gender, and 1005 and 1002 represent male and female, respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122df593",
   "metadata": {},
   "source": [
    "Now, let’s start by reading the dataset using pandas. That’s right, pandas is extremely good at handling data in a tabular format:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ed936e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebae507a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "n_rows =300000\n",
    "df = pd.read_csv(\"~/Downloads/train.csv\",nrows=n_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd94d10",
   "metadata": {},
   "source": [
    "The first 300,000 lines of the file are loaded and stored in a DataFrame. Take a quick look at the first five rows of the DataFrame:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95e400ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             id  click      hour    C1  banner_pos   site_id site_domain  \\\n",
      "0  1.000009e+18      0  14102100  1005           0  1fbe01fe    f3845767   \n",
      "1  1.000017e+19      0  14102100  1005           0  1fbe01fe    f3845767   \n",
      "2  1.000037e+19      0  14102100  1005           0  1fbe01fe    f3845767   \n",
      "3  1.000064e+19      0  14102100  1005           0  1fbe01fe    f3845767   \n",
      "4  1.000068e+19      0  14102100  1005           1  fe8cc448    9166c161   \n",
      "\n",
      "  site_category    app_id app_domain  ... device_type device_conn_type    C14  \\\n",
      "0      28905ebd  ecad2386   7801e8d9  ...           1                2  15706   \n",
      "1      28905ebd  ecad2386   7801e8d9  ...           1                0  15704   \n",
      "2      28905ebd  ecad2386   7801e8d9  ...           1                0  15704   \n",
      "3      28905ebd  ecad2386   7801e8d9  ...           1                0  15706   \n",
      "4      0569f928  ecad2386   7801e8d9  ...           1                0  18993   \n",
      "\n",
      "   C15  C16   C17  C18  C19     C20  C21  \n",
      "0  320   50  1722    0   35      -1   79  \n",
      "1  320   50  1722    0   35  100084   79  \n",
      "2  320   50  1722    0   35  100084   79  \n",
      "3  320   50  1722    0   35  100084   79  \n",
      "4  320   50  2161    0   35      -1  157  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0436bfd",
   "metadata": {},
   "source": [
    "The target variable is the click column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03301dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['click'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24b9296",
   "metadata": {},
   "source": [
    "For the remaining columns, there are several columns that should be removed from the features (id, hour, device_id, and device_ip) as they do not contain much useful information:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56f5964c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300000, 19)\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(['click','id','hour','device_id','device_ip'],axis=1).values \n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3c52ca",
   "metadata": {},
   "source": [
    "Each sample has 19 predictive attributes.\n",
    "\n",
    "Next, we need to split the data into training and testing sets. Normally, we do this by randomly picking samples. However, in our case, the samples are in chronological order, as indicated in the hour field. Obviously, we cannot use future samples to predict past ones. Hence, we take the first 90% as training samples and the rest as testing samples:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c50dc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(n_rows * 0.9)\n",
    "X_train = X[:n_train]\n",
    "Y_train = Y[:n_train]\n",
    "X_test = X[n_train:]\n",
    "Y_test = Y[n_train:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d52b2e",
   "metadata": {},
   "source": [
    "We initialize a OneHotEncoder object as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d929a455",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c43df14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x8204 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_enc = enc.fit_transform(X_train)\n",
    "X_train_enc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0872d9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1.0\n",
      "  (0, 6)\t1.0\n",
      "  (0, 188)\t1.0\n",
      "  (0, 2608)\t1.0\n",
      "  (0, 2679)\t1.0\n",
      "  (0, 3771)\t1.0\n",
      "  (0, 3885)\t1.0\n",
      "  (0, 3929)\t1.0\n",
      "  (0, 4879)\t1.0\n",
      "  (0, 7315)\t1.0\n",
      "  (0, 7319)\t1.0\n",
      "  (0, 7475)\t1.0\n",
      "  (0, 7824)\t1.0\n",
      "  (0, 7828)\t1.0\n",
      "  (0, 7869)\t1.0\n",
      "  (0, 7977)\t1.0\n",
      "  (0, 7982)\t1.0\n",
      "  (0, 8021)\t1.0\n",
      "  (0, 8189)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(X_train_enc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd30a5d7",
   "metadata": {},
   "source": [
    "Each converted sample is a sparse vector.\n",
    "\n",
    "We transform the testing set using the trained one-hot encoder as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8200c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_enc = enc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd9ef2c",
   "metadata": {},
   "source": [
    "We specified the handle_unknown='ignore' parameter in the one-hot encoder to prevent errors due to any unseen categorical values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74902874",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth': [3, 10, None]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dafd01e",
   "metadata": {},
   "source": [
    "We pick three options for the maximal depth – 3, 10, and unbounded. We initialize a decision tree model with Gini Impurity as the metric and 30 as the minimum number of samples required to split further:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12682332",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeClassifier(criterion='gini',\n",
    "                                        min_samples_split=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84969cc5",
   "metadata": {},
   "source": [
    "The classification metric should be the AUC(Area Under Curve) of the ROC(Receiver Operator Characteristic)curve, as it is an imbalanced binary case (only 51,211 out of 300,000 training samples are clicks, which is a 17% positive CTR(Click-Through Rate). As for grid search, we use three-fold (as the training set is relatively small) cross-validation and select the best-performing hyperparameter measured by the AUC:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33f6d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(decision_tree,parameters,n_jobs=-1,cv=3,scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470f1b8a",
   "metadata": {},
   "source": [
    "**Note:** n_jobs=-1 means that we use all of the available CPU processors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f03858b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 10}\n"
     ]
    }
   ],
   "source": [
    "grid_search.fit(X_train_enc, Y_train)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aefbf57",
   "metadata": {},
   "source": [
    "We use the model with the optimal parameter to predict any future test cases as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9f3f929",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_best = grid_search.best_estimator_\n",
    "pos_prob = decision_tree_best.predict_proba(X_test_enc)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea86e33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ROC AUC on testing set is: 0.719\n"
     ]
    }
   ],
   "source": [
    "print(f'The ROC AUC on testing set is: {roc_auc_score(Y_test, pos_prob):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5437e081",
   "metadata": {},
   "source": [
    "The AUC we can achieve with the optimal decision tree model is 0.72. This does not seem to be very high, but click-through involves many intricate human factors, which is why predicting it is not an easy task. Although we can further optimize the hyperparameters, an AUC of 0.72 is actually pretty good. As a comparison, randomly selecting 17% of the samples to be clicked on will generate an AUC of 0.499:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df72051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_prob = np.zeros(len(Y_test))\n",
    "click_index = np.random.choice(len(Y_test),int(len(Y_test)* 51211.0/300000),replace=False)\n",
    "pos_prob[click_index]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "189ed0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ROC AUC on testing set using random selection is:0.499\n"
     ]
    }
   ],
   "source": [
    "print(f'The ROC AUC on testing set using random selection is:{roc_auc_score(Y_test,pos_prob):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bee37e",
   "metadata": {},
   "source": [
    "Our decision tree model significantly outperforms the random predictor. Looking back, we can see that a decision tree is a sequence of greedy searches for the best splitting point at each step, based on the training dataset. However, this tends to cause overfitting as it is likely that the optimal points only work well for the training samples. Fortunately, ensembling is the technique to correct this, and random forest is an ensemble tree model that usually outperforms a simple decision tree.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8a86ff",
   "metadata": {},
   "source": [
    "# Ensembling decision trees – random forests\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771254fc",
   "metadata": {},
   "source": [
    "Random forest is a variant of the tree bagging model with additional feature-based bagging.\n",
    "\n",
    "To employ random forest in our click-through prediction project, we can use the package from scikit-learn. Similarly to the way we implemented the decision tree in the preceding section, we only tweak the **max_depth** parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cff0996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest = RandomForestClassifier(n_estimators=100,\n",
    "                                       criterion='gini',\n",
    "                                       min_samples_split=30,\n",
    "                                       n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498be818",
   "metadata": {},
   "source": [
    "Besides **max_depth**, **min_samples_split**, and **class_weight**, which are important hyperparameters related to a single decision tree, hyperparameters that are related to a random forest (a set of trees) such as **n_estimators** are also highly recommended. We fine-tune **max_depth** as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b73c2642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': None}\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(random_forest,parameters,n_jobs=1,cv=3, scoring='roc_auc')\n",
    "grid_search.fit(X_train_enc,Y_train)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e6e100",
   "metadata": {},
   "source": [
    "We use the model with the optimal parameter **None** for **max_depth** (the nodes are expanded until another stopping criterion is met) to predict any future unseen cases:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7fea8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ROC AUC on testing set using random forest is:0.759\n"
     ]
    }
   ],
   "source": [
    "random_forest_best = grid_search.best_estimator_\n",
    "pos_prob = random_forest_best.predict_proba(X_test_enc)[:, 1]\n",
    "print(f'The ROC AUC on testing set using random forest is:{roc_auc_score(Y_test,pos_prob):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b7f0a4",
   "metadata": {},
   "source": [
    "It turns out that the random forest model gives a substantial lift to the performance.\n",
    "Let’s summarize several critical hyperparameters to tune:\n",
    "\n",
    "- **max_depth:** This is the deepest individual tree. It tends to overfit if it is too deep or underfit if it is too shallow.\n",
    "\n",
    "- **min_samples_split:** This hyperparameter represents the minimum number of samples required for further splitting at a node. Too small a value tends to cause overfitting, while too large a value is likely to introduce underfitting. 10, 30, and 50 might be good options to start with.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95edff2a",
   "metadata": {},
   "source": [
    "The preceding two hyperparameters are generally related to individual decision trees. The following two parameters are more related to a random forest or collection of trees:\n",
    "\n",
    "- **max_features:** This parameter represents the number of features to consider for each best splitting point search. Typically,for an *m*-dimensional dataset,$$\\sqrt{m}$$(rounded) is a recommended value for **max_features**. This can be specified as **max_features=\"sqrt\"** in scikit-learn. Other options include **log2**, 20%, and 50% of the original features.\n",
    "- **n_estimators:** This parameter represents the number of trees considered for majority voting. Generally speaking, the more trees, the better the performance but the longer the computation time. It is usually set as 100, 200, 500, and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7ef9c5",
   "metadata": {},
   "source": [
    "# Ensembling decision trees – gradient-boosted trees\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde08fed",
   "metadata": {},
   "source": [
    "Boosting, which is another ensemble technique, takes an iterative approach instead of combining multiple learners in parallel. In boosted trees, individual trees are no longer trained separately. Specifically, in **Gradient-Boosted Trees (GBT)** (also called **Gradient-Boosting Machines**), individual trees are trained in succession where a tree aims to correct the errors made by the previous tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df0d0c8",
   "metadata": {},
   "source": [
    "The random forest model builds each tree independently using a different subset of the dataset, and then combines the results at the end by majority votes or averaging.\n",
    "\n",
    "The GBT model builds one tree at a time and combines the results along the way\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b3447",
   "metadata": {},
   "source": [
    "GBT works by iteratively improving the ensemble’s predictions through the addition of sequentially trained decision trees, with each tree focusing on the residuals of the previous ones. Here’s how it works:\n",
    "\n",
    "**Initialization:** The process starts with an initial simple model, often a single decision tree, which serves as the starting point for the ensemble.\n",
    "\n",
    "**Sequential training:** Subsequent decision trees are trained sequentially, with each tree attempting to correct the errors of the previous ones. Each new tree is trained on the residuals (the differences between the actual and predicted values) of the ensemble’s predictions from the previous trees.\n",
    "\n",
    "**Additive modeling:** Each new decision tree is added to the ensemble in a way that minimizes the overall error. The trees are typically shallow, with a limited number of nodes, to avoid overfitting and improve generalization.\n",
    "\n",
    "**Learning rate:** GBT introduces a learning rate parameter, which controls the contribution of each tree to the ensemble. A lower learning rate leads to slower learning but can enhance the overall performance and stability of the ensemble.\n",
    "\n",
    "**Ensemble prediction:** The final prediction is made by combining the predictions of all the trees in the ensemble.”\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8587d53d",
   "metadata": {},
   "source": [
    "Let’s now take a look at the following steps. You will see how we predict clicks using GBT:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12347067",
   "metadata": {},
   "source": [
    "**1.** We import XGBoost and initialize a GBT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc77396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "model = xgb.XGBClassifier(learning_rate=0.1,max_depth=10,\n",
    "                         n_estimators=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21094cb",
   "metadata": {},
   "source": [
    "We set the learning rate to 0.1, which determines how fast or slow we want to proceed with learning in each step (in each tree, in GBT)\n",
    "\n",
    "**Max_depth** for individual trees is set to 10. \n",
    "\n",
    "Additionally, 1,000 trees will be trained in sequence in our GBT model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed2e2ff",
   "metadata": {},
   "source": [
    " **2.** Next, we train the GBT model on the training set we prepared previously:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b118090a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=1000, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=1000, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=1000, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_test_enc,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deedbbf",
   "metadata": {},
   "source": [
    "**3.** We use the trained model to make predictions on the testing set and calculate the ROC AUC accordingly:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f43d5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ROC AUC on testing set using GBT is:0.498\n"
     ]
    }
   ],
   "source": [
    "pos_prob = model.predict_proba(X_test_enc)[:, 1]\n",
    "print(f'The ROC AUC on testing set using GBT is:{roc_auc_score(Y_test,pos_prob):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75435054",
   "metadata": {},
   "source": [
    "We are able to achieve 0.498 AUC using the XGBoost GBT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a73e26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
